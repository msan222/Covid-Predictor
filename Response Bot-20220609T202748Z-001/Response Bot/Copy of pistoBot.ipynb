{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DUf8H8xQxMSR"},"outputs":[],"source":["import gdown             "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1634003578235,"user":{"displayName":"Madeline Sanvictores","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcgnFbNwJ9DsseRpFuEsZwwxSDLj6i_IAD5MkBDg=s64","userId":"08914533343715343326"},"user_tz":420},"id":"VRSi2GsHxo3w","outputId":"458dbf4f-8682-4200-d98f-ac8ecdf1e04b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1f4vOzkZfcqzEAo-yujV0KNpGXUsCeQ_W\n","To: /content/test.kismet\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.8k/13.8k [00:00<00:00, 2.42MB/s]\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'test.kismet'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["gdown.download('https://drive.google.com/uc?id=1f4vOzkZfcqzEAo-yujV0KNpGXUsCeQ_W','test.kismet',quiet=False)\n","\n","#https://drive.google.com/file/d/1f4vOzkZfcqzEAo-yujV0KNpGXUsCeQ_W/view?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u87s9jzOzVTa"},"outputs":[],"source":["!mkdir -p ./data/chat_raw/whatsapp\n","!mv test.kismet /content/data/chat_raw/whatsapp"]},{"cell_type":"markdown","metadata":{"id":"wMBML9lTaLva"},"source":["# **ü§ñ pistoBot**\n","\n","> Create an AI that (try) to chat like you.<br>\n","> by [Simone Guardati](https://www.linkedin.com/in/simone-guardati/)\n","\n","\n","## ü•ú In a nutshell\n","1. Get your whatsapp and telegram data\n","2. Parse it to get a ML-like dataset\n","3. Train a GTP2 model\n","4. Chat with the model\n","\n","**üîó Resources**\n","- Chat parser - [github](https://github.com/pistocop/messaging-chat-parser)\n","- pistoBot - [github](https://github.com/pistocop/pistoBot)\n","- pistoBot website - [link](https://pistocop.github.io/pistoBot-website/)\n","\n","<br>\n","\n","**‚ö†Ô∏è Warning**\n","- It's always better not to run random scripts on personal information (like personal chat messages).\n","- I guarantee there is no double end, but you can always check (and use) the native code: <br>\n","[messaging-chat-parser](https://github.com/pistocop/messaging-chat-parser) and [pistobot](https://github.com/pistocop/pistoBot)\n"]},{"cell_type":"markdown","metadata":{"id":"7GLofGtbgR5c"},"source":["------\n","## 0Ô∏è‚É£ Init"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1634003682121,"user":{"displayName":"Madeline Sanvictores","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcgnFbNwJ9DsseRpFuEsZwwxSDLj6i_IAD5MkBDg=s64","userId":"08914533343715343326"},"user_tz":420},"id":"9Zasps9vfHk0","outputId":"c748cdb6-59c9-47ce-94c4-21227384a575"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Oct 12 01:54:42 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi # p100 suggested"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3770,"status":"ok","timestamp":1633728813328,"user":{"displayName":"Madeline Sanvictores","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcgnFbNwJ9DsseRpFuEsZwwxSDLj6i_IAD5MkBDg=s64","userId":"08914533343715343326"},"user_tz":420},"id":"Clix8N59cqqL","outputId":"a4550445-e910-47c2-94dd-0d01951d91c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'messaging-chat-parser' already exists and is not an empty directory.\n","fatal: destination path 'pistoBot' already exists and is not an empty directory.\n"]}],"source":["import os\n","!git clone --quiet https://github.com/pistocop/messaging-chat-parser.git\n","!pip install -q -r messaging-chat-parser/requirements.txt\n","!git clone --quiet https://github.com/pistocop/pistoBot.git"]},{"cell_type":"markdown","metadata":{"id":"6qEkk-YZbDN_"},"source":["-----\n","## 1Ô∏è‚É£ Get the data\n","> Get your chat from whatsapp and telegram and upload them under `./messaging-chat-parser/data/chat_raw/` notebook folder\n","\n","\n","- **WhatsApp**\n","    - _.txt_ files exported from one or more chat - [how](https://faq.whatsapp.com/en/android/23756533/) (under \"Export chat history\" section)\n","        - don't export images\n","        - don't export group chats\n","    - place all txt files in `./messaging-chat-parser/data/chat_raw/whatsapp/*.txt`\n","- **Telegram** \n","    - _.json_ with the telegram dump - [how](https://telegram.org/blog/export-and-more)\n","        - don't export images\n","        - choose \"machine-readable JSON\"\n","    - copy and rename the json file in `./messaging-chat-parser/data/chat_raw/telegram/telegram_dump.json`\n","\n","\n","------"]},{"cell_type":"markdown","metadata":{"id":"hq_6S7lCemLW"},"source":["## 2Ô∏è‚É£ Parse the data\n","\n","**Whatsapp**\n","- Set the following variable `whatsapp_user_name`\n","    - Get it from one of the WhatsApp chat exported text. \n","    <br> e.g. from one line: <br> \n","     _12/12/19, 08:40 - `whatsapp_user_name`: bla bla bla_ \n","- Datetime:\n","    - WhatsApp and Telegram have two different ways to manage the datetime.\n","    - Here are listed the two format, with Italian default values, if your data have different formats, change accordingly the next line values\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHSOoxejg8s4"},"outputs":[],"source":["whatsapp_user_name = \"Maddie_22\" # <--- your name, extracted from Whatsapp data\n","whatsapp_datetime_format = \"%m/%d/%y, %H:%M %p\" # <-- American format used (MDY)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3_N2WUso_XI"},"outputs":[],"source":["# Whatsapp\n","print(\"> [WHATSAPP] start parsing...\")\n","assert whatsapp_user_name is not None, \"[!] Whatsapp user name not setted\"\n","!cd messaging-chat-parser && python ./src/whatsapp_parser.py --session_token \"<|endoftext|>\" --user_name $whatsapp_user_name --time_format \"$whatsapp_datetime_format\"\n","print(\"> [WHATSAPP] parsing completed!\\n\\n\")\n","print(\"----------------------------------\")\n","\n","# Join Telegram and Whatsapp data\n","!cd messaging-chat-parser/ && python ./src/joiner.py\n","training_data_lines = sum(1 for line in open('./messaging-chat-parser/data/chat_parsed/all-messages.txt'))\n","print(f\"> [PARSER] Training file lines: {training_data_lines}\")\n","print(\"----------------------------------\")\n","\n","# Check data size\n","if training_data_lines < 1000:\n","    print(f\"[WARNING] attention insufficient training data ({training_data_lines} < 100K), it is recommended to export more chats\")"]},{"cell_type":"markdown","metadata":{"id":"cDiyALCfsOwq"},"source":["------\n","## 3Ô∏è‚É£ Train a GTP2 model\n","\n","- ‚è≥ The following cell could take **up to 10 hours**\n","    - An estimation of the total time will be prompted\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24123,"status":"ok","timestamp":1633729149781,"user":{"displayName":"Madeline Sanvictores","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcgnFbNwJ9DsseRpFuEsZwwxSDLj6i_IAD5MkBDg=s64","userId":"08914533343715343326"},"user_tz":420},"id":"qwHVtFvPfBXg","outputId":"eb587e2d-2e4d-456c-c123-0d26049f6c2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing common requirements...\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 269 kB 5.4 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 33.7 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 495 kB 45.7 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43 kB 1.8 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 2.6 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.6 MB 32.1 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 404 kB 48.2 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 428 kB 42.3 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 5.2 MB/s \n","\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n","[gpt2-scratch model chosen]\n","Installing requirements...\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 248 kB 5.4 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 569 kB 36.9 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829 kB 33.3 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 674 kB 20.3 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 6.4 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 33.9 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.6 MB 23.5 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 38.0 MB/s \n","\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Training model...\n","WARNING:root:Keys file at ./data/inputs/personal/my-keys.txt not found\n","INFO:root:Training tokenizer...\n","[00:00:00] Reading files                            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                   0\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 100\n","[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n","thread '<unnamed>' panicked at 'assertion failed: chunk_size != 0', /rustc/7f3df5772439eee1c512ed2eb540beef1124d236/src/libcore/macros/mod.rs:34:9\n","note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n","fatal runtime error: failed to initiate panic, error 5\n","run_training.sh: line 37:   238 Aborted                 (core dumped) python ./pistoBot/03_gpt2_scratch/gpt2_scratch.py -v\n"]}],"source":["!cp ./messaging-chat-parser/data/chat_parsed/all-messages.txt ./pistoBot/data/inputs/chat_parsed/all-messages-endoftext.txt\n","!cd pistoBot/colab/ && bash run_training.sh gpt2-scratch"]},{"cell_type":"markdown","metadata":{"id":"a94toYckhHIR"},"source":["---\n","## 4Ô∏è‚É£ Chat with the model\n","\n","Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":241,"status":"error","timestamp":1633576563116,"user":{"displayName":"Madeline Sanvictores","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcgnFbNwJ9DsseRpFuEsZwwxSDLj6i_IAD5MkBDg=s64","userId":"08914533343715343326"},"user_tz":420},"id":"H-ivHvsOhaNc","outputId":"b6bb7ca3-12e3-43a7-86c2-efe22404c853"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:aitextgen:Constructing GPT-2 model from provided config.\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-68c6184c3c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                \u001b[0mmerges_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerges_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                to_gpu=True)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config, vocab_file, merges_file, cache_dir, tf_gpt2, to_gpu, to_fp16, verbose, torchscript, ts_to_trace, bos_token, eos_token, unk_token, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Manually construct a GPT-2 model from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Constructing GPT-2 model from provided config.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# Download and cache model from Huggingface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;34m\"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \"Model type should be one of {}.\".format(\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_WITH_LM_HEAD_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             )\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'str'> for this kind of AutoModel: AutoModelWithLMHead.\nModel type should be one of T5Config, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaConfig, MarianConfig, BartConfig, LongformerConfig, RobertaConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, FlaubertConfig, XLMConfig, CTRLConfig, ElectraConfig, EncoderDecoderConfig, ReformerConfig."]}],"source":["from aitextgen import aitextgen\n","from pprint import pprint\n","\n","files = os.listdir(\"./pistoBot/data/models_trained/\")\n","files.remove('.gitkeep')\n","folder_name = files[0]\n","\n","model_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name, \"pytorch_model.bin\")\n","config_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"config.json\")\n","vocab_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"aitextgen-vocab.json\")\n","merges_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"aitextgen-merges.txt\")\n","\n","ai = aitextgen(model=model_path, \n","               config=config_path,\n","               vocab_file=vocab_path,\n","               merges_file=merges_path,\n","               to_gpu=True)"]},{"cell_type":"markdown","metadata":{"id":"wbLNJlQokvYf"},"source":["### üí¨ Interactive mode\n","> Chat with the model one message at a time\n","\n","- Run the following cell and use the prompt (‚úç) to write your messages\n","- The chats messages will show two tags:\n","    - **[others]** tags: messages wrote by the user\n","    - **[me]** tags: messages generated by the model\n","\n","<br>\n","\n","- Error _Max temperature reached_:\n","    - **Solution**: re-run the cell\n","    - Motivation: under the hood the program increase the _temperature_ value to get a new message that start with \"[me]\" tag. This is done until a max value is reached.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sBwemFaak1kV"},"outputs":[],"source":["chat = []\n","start_temperature = 0.7\n","max_temperature = 3.0\n","\n","for _ in range(5):\n","    new_line = \"[others] \" + input(\"‚úç\") + '\\n'\n","    chat.append(new_line)\n","    \n","    me_token = False\n","    temperature = start_temperature\n","    input_network = ' '.join(chat)\n","    \n","    while not me_token:\n","        text = ai.generate(prompt=input_network, \n","                           return_as_list=True, \n","                           temperature=temperature)\n","        text = text[0] # batch of 1\n","\n","        text = text.split('\\n')\n","        chat_pos = len(chat)\n","        network_reply = text[chat_pos]\n","\n","        if network_reply.startswith('[me]'):\n","            me_token = True\n","            network_reply = text[chat_pos] + '\\n'\n","            chat.append(network_reply)\n","        else:\n","            if temperature >= max_temperature:\n","                raise RuntimeError(\"Max temperature reached\")\n","            temperature += 0.1\n","    # print(f'temperature exit: {temperature}')\n","    print('Chat:')\n","    pprint(chat)\n","    print('---------------------')\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of pistoBot.ipynb","provenance":[{"file_id":"1T4-Gk-mlAWJkX9RuRd3_EiS5JBP5UvyV","timestamp":1633576750147}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}